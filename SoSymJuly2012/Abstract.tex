\begin{abstract}

	
	Testing a \emph{model transformation} requires input test models which effectively cover the input domain of the transformation. In order to reduce testing costs and increase error-revealing power of test models, it is necessary to automate the generation of these models using systematic criteria.This automation faces two key challenges: (1) accurate specification of  the  transformation's input domain (2) automatic generation of  test models in the input domain based on effective coverage criteria. This paper presents a global approach that addresses these challenges. Typically the input domain defines a possibly infinite set of models specified using various sources of knowledge: the input metamodel of the transformation, static semantic constraints on this metamodel and pre-conditions that further constrain the input domain for a particular transformation. We use our tool {\Pramana} to automatically generate a finite number of test models in the input domain using coverage criteria based on partitioning properties of the input metamodel. Testing the transformation with these models often reveals that some test models are not executable by the transformation although they satisfy the constraints of the input domain specification. This gives way to incremental refinement of the input domain specification using newly constructed pre-conditions until all generated test models are executable by the transformation. In our experiments, we validate the proposed approach with mutation analysis to empirically evaluate error-revealing power of the test models we generate using {\Pramana}. The empirical evaluation uses the representative transformation of simplified UML class diagram models to RDBMS models. The evaluation is based on  3200 automatically generated test models. We demonstrate that partitioning strategies gives mutation scores of up to 93\% vs. 72\% in the case of unguided generation.

\end{abstract}
