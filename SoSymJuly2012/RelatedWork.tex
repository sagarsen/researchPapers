\section{Related Work}
\label{sec:RelatedWork}


We explore three main areas of related work : test criteria, automatic test generation, and qualification of strategies. 

The first area we explore is work on test criteria in the context of model transformations in {\MDE}. Random generation and input domain partitioning based test criteria are two widely studied and compared  strategies in software engineering (non {\MDE})  \cite{vagoun1996}  \cite{elaine1991} \cite{Gutjahr99partitiontesting}. To extend such test criteria to {\MDE} we have presented in \cite{franck2007} input domain partitioning of input meta-models in the form of model fragments. However, there exists no experimental or theoretical study to qualify the approach proposed in \cite{franck2007}.

Experimental qualification of the test strategies require techniques for automatic model generation. Model generation is more general and complex than generating integers, floats, strings, lists, or other standard data structures such as dealt with in the Korat tool of Chandra et al. \cite{chandra2002}. Korat is faster than {\Alloy} in generating data structures such as binary trees, lists, and heap arrays from the Java Collections Framework but it does not consider the general case of models which are arbitrarily constrained graphs of objects. The constraints on models makes model generation a different problem than generating test suites for context-free grammar-based software \cite{hen2005} which do not contain domain-specific constraints. 

  Test models are complex graphs that must conform to an input meta-model specification, a transformation pre-condition and additional knowledge such as model fragments to help detect bugs. In \cite{brottier2006} the authors present an automated generation technique for models that conform only to the class diagram of a meta-model specification. A similar methodology using graph transformation rules is presented in \cite{ehrig2006}. Generated models in both these approaches do not satisfy the constraints on the meta-model. In \cite{sen2007} we present a method to generate models given partial models by transforming the meta-model and partial model to a {\textCLP} ({\CLP}). We solve the resulting {\CLP} to give model(s) that conform to the input domain. However, the approach does not add new objects to the model. We assume that the number and types of models in the partial model is sufficient for obtaining complete models. The constraints in this system are limited to first-order horn clause logic.  In \cite{sen2008} we have introduce a tool {\Cartier} based on the constraint solving system {\Alloy} to resolve the issue of generating models such that constraints over both objects and properties are satisfied simultaneously. In this paper we use {\Cartier} to systematically generate several hundred models driven by knowledge/constraints of model fragments \cite{franck2007}. Statistically relevant test model sets are generated from a factorial experimental design \cite{shari2005} \cite{walter1955}.% \cite{hoskins2004}.

The qualification of a set of test models can be based on several criteria such as code and rule coverage for white box testing, satisfaction of post-condition or mutation analysis for black/grey box testing. In this paper we are interested in obtaining the relative adequacy of a test set using mutation analysis \cite{demillo1978}. In previous work \cite{mottu2006} we extend mutation analysis to {\MDE} by developing mutation operators for model transformation languages. We qualify our approach using a representative transformation {\UMLCD} models to {\RDBMS} models called {\transfo} implemented in the transformation language Kermeta \cite{muller2005}. This transformation \cite{bezivin2005} was proposed in the MTIP Workshop in MoDeLs 2005 as a comprehensive and representative case study to evaluate model transformation languages.
