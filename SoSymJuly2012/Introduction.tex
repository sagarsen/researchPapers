\section{Introduction}
\label{section:Introduction}

Model transformations are core {\textMDE} ({\MDE}) components that automate important steps in software development such as refinement of an input model, re-factoring to improve maintainability or readability of the input model, aspect weaving into models, exogenous/endogenous transformations of models, and the classical generation of code from models. Although there is wide spread development of model transformations in academia and industry. However, there is mild progress in techniques to test transformations \cite{Baudry09a}. In this paper, we address the problem of testing model transformations using automatically generated  test models. Our approach is applicable to a diverse set of transformation languages such as those based on graph rewriting \cite{bardohl99}, imperative execution (Kermeta \cite{muller2005}), and rule-based transformation (ATL \cite{jouault2006}).

   
 Testing a model transformation requires a \emph{set of test models} in the input domain of a model transformation.   The automatic synthesis/generation of such test models that can reveal bugs in a transformation is the subject of this paper. 
 
 The automatic generation of test models selects test models from the set of all input models. This set of input models is precisely specified by the input domain of a model transformation. Typically, this input domain specification relies on knowledge from various sources: (1) the input metamodel of the transformation (2) invariants/constraints on the static semantics of the input metamodel (3) pre-condition contracts for a particular transformation.  We call the cumulative set of these sources of knowledge the \emph{input domain model}.  The input domain model  specifies potentially an infinite set of  input models. Therefore, we need to go a step further and define effective strategies, such as coverage criteria  \cite{franck2007}, that can help automatically select a finite number of  test models in the infinite set.  

In this paper, we build on previous work \cite{sen2008} to automatically generate test models using the tool {\Pramana} (previously known as {\Cartier}) within the input domain of a transformation. Executing the model transformation with these test models often results in some test models being rejected by the transformation. Although these test models conform to the initial specification of the input domain model, they are rejected by the transformation. This may happen when the transformation runs into an infinite cyclic loop while navigating an input model. At this point there are two possibilities: (a) Modifying a model transformation's specification and consequently its implementation to handle such an input model (b) Creating a pre-condition that avoids input models with a certain pattern such as a loop. This is a dilemma about whether to correct a model transformation to make it robust or improve the pre-condition to handle unforeseen inputs. The choice of one approach or the other depends on how we would like to interprets the specification. In this paper, we consider taking step (b) to identify the \emph{true input domain} of a model transformation conforming to a specification.  Our \emph{first contribution} in the paper is as follows:

\textbf{Incremental Pre-condition Improvement}: We automatically generate test models using {\Pramana} to first identify test models outside the unknown \emph{true} input domain of a model transformation. We  systematically compose new pre-conditions using information  extracted from a  malicious pattern in the generated test models. We improve the current set of pre-conditions of the model transformation. We use the new specification of the input domain model to generate test models and improve the set of pre-conditions. We continue the process until no new pre-condition is required and all generated test models can be executed by the transformation. The output of the process is the precise model of the true input domain of a model transformation. Pre-condition improvement is an approach adapted to black-box transformations where we have no access to the implementation. It also reveals the unforeseen requirements in case we want to evolve a model transformation. Therefore, a pre-condition may be converted to a new model transformation feature. We see pre-condition improvement as either a means to protect the model transformation or to extract requirements in the form of constraints/rules for the evolution of the transformation.

Once, we have a precise input domain model the consequent next step is to generate new test models. This time to detect bugs in a transformation. This brings us to the \emph{second contribution} of the paper:

\textbf{Automatic Generation and Evaluation of Test Models}: We first use {\Pramana} to generate sets of  test models for different strategies in the precise input domain of a model transformation. In this paper, we generate test models that satisfy coverage criteria \cite{franck2007}.  Second, we use \emph{mutation analysis} \cite{demillo1978} \cite{mottu2006} for model transformations as technique to evaluate if these test sets can indeed reveal bugs. Mutation analysis consists of artificially injecting \emph{model transformation specific} bugs into a model transformation giving a set of mutant model transformations with exactly one bug in each mutant. We execute each of these mutants with a set  of generated test models for a given strategy. A difference in output between the original transformation and a mutant transformation reveals that an error was detected for the same input test model. Consequently, the number of such errors detected by a set of test model refers to the \emph{mutation score} of the test set. This number of errors detected corresponds to the \emph{mutation score} of a test set which is the metric we use to compare test generation strategies. Mutation analysis is done mainly to evaluate the effectiveness of our coverage criteria to generate test models that detect bugs. However, mutation analysis is not used in practice for testing. We use mutation analysis to gain confidence that test models generated with {\Pramana} can detect bugs for the representative transformation. Based on our mutation analysis results we encourage  generation of test models using our coverage criteria in new scenarios involving arbitrary input domains and transformations.

We demonstrate incremental pre-condition improvement and empirically evaluate automatic test model generation for  the  representative model transformation of {\textUMLCD} ({\UMLCD}) to {\textRDBMS}  ({\RDBMS}) models called {\transfo}.  We discover nine new pre-conditions for the {\transfo}. Using mutation analysis we demonstrate that  our input domain coverage strategies, previously presented in \cite{franck2007}, can  select test models with considerably higher bug detection abilities (93\%) compared to unguided selection (72\%) in the input domain.. These results are based on 3200 generated test models and several hours of computation on a 10 machine grid of high-end servers.  The large difference in mutation scores between coverage strategies and unguided selection can be attributed to  the fact that coverage strategies enforce several aspects on test models that unguided selection fails to do. For instance, coverage strategies enforce injection of \emph{inheritance} in the {\UMLCD} test models. Unguided strategies do not enforce such a requirement. Several mutants are killed due to test models containing inheritance. 
	 

The paper is organized as follows. In Section \ref{sec:ProblemDescription} we present the transformation testing problem and the running case study. In Section \ref{sec:Foundation}, we present foundational ideas used in {\Pramana}. In  Section \ref{sec:Methodology}, we describe the {\Pramana} methodology for incremental pre-condition improvement and the empirical approach for automatic test model generation. In Section \ref{sec:Experiments}, we present the experimental setup for test model generation using different strategies and discuss the results of mutation analysis. In Section \ref{sec:RelatedWork} we present related work. We conclude in Section \ref{sec:Conclusion}.
