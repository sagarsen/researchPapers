\section{Introduction}
\label{section:Introduction}

Model transformations are core {\textMDE} ({\MDE}) components that can automate important software development steps such as model refinement, model refactoring to improve maintainability, readability, aspect weaving, and generation of code from models. Although there is wide spread development of model transformations in academia and industry there is mild progress in the domain of validating transformations \cite{Baudry09a}. In this paper, we address the challenge of automatic generation of models for model transformation testing. We choose black-box testing approach in order to propose an effective approach for testing transformations in languages founded on different paradigms such as graph rewriting \cite{bardohl99}, imperative execution (Kermeta \cite{muller2005}), and rule-based (ATL \cite{jouault2006}).

The automatic generation of test models faces two major challenges: (i) automating the generation of valid models that can be processed by the model transformation; (ii) the selection of effective  models that can detect bugs in the transformation. This paper builds on previous work \cite{sen2008} to propose a global approach for  automatic generation of models to test model transformations. We empirically validate the effectiveness of the generated models using mutation analysis.

The automatic generation of valid models relies on an accurate specification of the input domain for the transformation. This specification is usually provided in the form of the source metamodel of the transformation. However, the source metamodel usually specifies a set of models that is larger than the actual input domain of the transformation:. All valid input models are in this set, nevertheless there exist models in the set that are not valid inputs to the transformation. There are several reasons why the source metamodel is not precise specification of the input domain. 
%The first reason comes from the limitations of current metamodeling languages such as MOF to specify all necessary constraints over a domain. MOF allows the definition of concepts and properties but this is not enough to express all structural invariants over the modeling domain. Thus, it is usually necessary to add invariants in OCL for example.
One reason is that a large number of metamodels are standards such as the {\UML}. Thus, it is important to consider them as source for a transformation in order to ensure compatibility with other transformations and tools. For instance, the {\UML} is standard source metamodel for most transformations. do not deal with whole UML. Thus a transformation that declares UML as its source metamodel usually has a much smaller input domain defined over a subset of UML concepts. Another reason why a metamodel is not completely accurate is that most transformations make specific assumptions about the models they get as input. These assumptions cannot be enforced by the source metamodel since they are specific to the transformation. They should thus be declared as pre conditions on the input domain of the transformation. For example, a transformation over UML statecharts might assume non-hierarchical statecharts. In that case, the source metamodel is UML and it is necessary to add a precondition that prevents hierarchical input statecharts in order to have a precise specification of the input domain.

In this paper we introduce an global process that starts from an initial source metamodel and that increases the accuracy of the input domain specification. This process uses the previously developed metamodel pruning algorithm \cite{sen2009b} in order to extract a consistent subpart of the source metamodel. Then we propose to assist the definition of pre conditions for the transformation through an incremental exploration process that consists in generating models, running the transformation with these models, and when the transformation crashes, manually look for a pre condition that can prevent such input model.

On the basis of an accurate specification of the input domain, we can automatically generate valid models \cite{sen2008} and use them as test data for the transformation. However, the input domain is usually infinitely large. Thus, test generation consists in selecting a subset of all valid model that can reveal errors in the transformation. In this paper we experiment the ability of test criteria defined by Fleurey et al. \cite{franck2007} to drive the automatic generation towards a set of test models that can detect errors. In this paper we compare the generation based on these test criteria with a generation that is completely unguided (all we know is that it generates valid models). The comparison is performed through an empirical analysis of the different automatic generation strategies. This experiment is based on mutation analysis \cite{demillo1978}\cite{mottu2006} which consists in injecting errors in a transformation and checking whether the input models can detect them.

The whole approach has been implemented in a tool called \emph{Pramana} which is based on {\Alloy} \cite{daniel2006,sen2008} for automatic exploration. We automatically generate 3200 test models according to different strategies, and over different set of test models, for  the  representative model transformation of {\textUMLCD} ({\UMLCD}) to {\textRDBMS} \\ ({\RDBMS}) models called {\transfo}.  The mutation scores show that test criteria guide model generation with considerably higher bug detection abilities (93\%) compared to unguided generation (70\%).  The large difference in mutation scores between coverage strategies and unguided generation can be attributed to  the fact that coverage strategies enforce several aspects on test models that unguided generation fail to do. For instance, coverage strategies enforce injection of \emph{inheritance} in the {\UMLCD} test models. Unguided strategies do not enforce such a requirement. Several mutants are killed due to test models containing inheritance. These experiments also indicate that \emph{Pramana} can generate sets of models that have a consistent ability to detect errors. This means that, even though \emph{Pramana} and {\Alloy} make some random choices during the generation of models, the final set of model has a relatively constant quality from generation to the other. This also means that the ability of  \emph{Pramana} to consider test criteria to drive the generation of models has an actual impact on the quality of the models over {\Alloy}'s choices for constraint solving.	 

%The paper is organized as follows. In Section \ref{sec:ProblemDescription} we present the transformation testing problem and the running case study. In Section \ref{sec:Foundation}, we present foundational ideas used in {\Cartier}. In  Section \ref{sec:Methodology}, we describe the {\Cartier} methodology for automatic test model generation. In Section \ref{sec:Experiments}, we present the experimental setup for test model generation using different strategies and discuss the results of mutation analysis. In Section \ref{sec:RelatedWork} we present related work. We conclude in Section \ref{sec:Conclusion}.


%Model transformations are core {\MDE} components that automate important steps in software development such as refinement of an input model, re-factoring to improve maintainability or readability of the input model, aspect weaving, exogenous and endogenous transformations of models, and generation of code from models. Although there is wide spread development of model transformations in academia and industry the validation of transformations remains a hard problem \cite{Baudry09a}. In this paper, we address the challenges in validating model transformations via \emph{black-box automatic test data generation}.  We think that black-box testing is an effective approach to validating transformations due to the diversity of transformation languages based on graph rewriting \cite{bardohl99}, imperative execution (Kermeta \cite{muller2005}), and rule-based transformation (ATL \cite{jouault2006}) that render language specific formal methods and white-box testing currently impractical.

%In black-box testing of model transformations we require \emph{test models} that can \emph{detect bugs} in the model transformation. These models are graphs of inter-connected objects that must conform to a meta-model and satisfy meta-constraints such as well-formedness rules, transformation pre-conditions, and test strategies. Manually specifying several hundred test models targeting various testing objectives is a tedious task and in many cases impossible since the modeller may have to simultaneously satisfy numerous possibly inter-related constraints. 

%In this paper, we present the tool and framework {\Cartier} for \emph{automatic test model generation} based on the general idea of \emph{constraint satisfaction} in the domain of models. {\Cartier} has to address two main problems for test generation: identify a precise model of the transformation's input domain; automatically select relevant test models in the input domain. The first issue is related to the fact that the input domain of a transformation is generally described with a general purpose metamodel (e.g., {\UML}). However, the effective input domain, that captures only the set of models that can be transformed, is much smaller than the set of instances of the general purpose metamodel.  {\Cartier} can prune the metamodel in order to explicitly build the sub part of the metamodel that the transformation can manipulate.  {\Cartier} also assists the definition of pre conditions on the metamodel to make the input domain more precise. Once the input domain is precisely modelled,  {\Cartier} can generate model from this domain.  {\Cartier} either generate any model without guidance or it can use test strategies in order to have models that cover the input domain \cite{franck2007}. 

%We briefly describe {\Cartier}'s approach in three steps. In the first step, {\Cartier} derives a precise specification of the model transformation's input domain via  \emph{meta-model pruning}. Meta-model pruning obtains an effective input meta-model from the input meta-model of the transformation such that it contains only concepts relevant to the transformation. In the second step, {\Cartier} transforms knowledge from heterogenous sources such as the effective input meta-model, well-formedness rules, initial set of pre-conditions, and test strategies to a unified system of constraints. These constraints are generated in a common formal specification language {\Alloy} \cite{daniel2006}. In step three, {\Cartier} solves the {\Alloy} model to generate finite size test models. These test models may help refine input domain of the model transformation by discovery of new pre-condition constraints. These pre-conditions correspond to unforeseen malicious structures such as those leading to navigation cycles. These structural patterns often escape the perception of expert transformation designers. Once we discover all possible pre-condition constraints we execute {\Cartier} to re-generate test models that  simultaneously conform to knowledge from all heterogeneous sources.

%Are the test models generated by {\Cartier} able to detect bugs in a model transformation? We answer this question by generating and comparing sets of test models using different testing strategies. Specifically, we consider two testing strategies: \emph{unguided} and \emph{input domain coverage strategies}  \cite{franck2007}.   We use  \emph{mutation analysis}  \cite{demillo1978} \cite{mottu2006} for model transformations to compare these testing strategies. Mutation analysis serves as a \emph{test oracle} to determine the relatively adequacy of generated test sets.

%We perform experiments to generate test models using different testing strategies and qualify them using mutation analysis. We generate test models for  the  representative model transformation of {\textUMLCD} ({\UMLCD}) to {\textRDBMS} \\ ({\RDBMS}) models called {\transfo}.  The mutation scores show that input domain coverage strategies guide model generation with considerably higher bug detection abilities (93\%) compared to unguided generation (70\%). These results are based on 3200 generated test models and several hours of computation on a 10 machine grid of high-end servers.  The large difference in mutation scores between coverage strategies and unguided generation can be attributed to  the fact that coverage strategies enforce several aspects on test models that unguided generation fail to do. For instance, coverage strategies enforce injection of \emph{inheritance} in the {\UMLCD} test models. Unguided strategies do not enforce such a requirement. Several mutants are killed due to test models containing inheritance. 

%The \emph{scientific contribution} in this paper is based on a combination of several recently published ideas. In \cite{sen2008}, the authors for the first time present {\Cartier} demonstrating the possibility of automatically generating a variety of test models. In \cite{Sen09a}, the authors generate hundreds of test models using different strategies to show that automatically generated test models using partitioning strategies can indeed detect bugs. We validate the bug detecting effectiveness of the generated test models using mutation analysis of model transformations \cite{mottu2006}. However, three important questions remain:

%\begin{itemize}
%	\item \textbf{Question 1:} How can we scale the approach to generating test models for large input meta-models such the {\UML}?
%	\item \textbf{Question 2:} Does the model transformation pre-condition precisely specify the input domain of a model transformation? If not, can automatically generated test models help improve the pre-condition by presenting unforeseen and unwanted modelling patterns?
%	\item \textbf{Question 3:} Are we consistently able to generate effective test models for a given strategy using our approach? 
%\end{itemize}

%The precise contributions of this paper address exactly these problems. We enlist them below:

%\begin{itemize}
%	\item \textbf{Contribution 1:} We apply the recently proposed \emph{meta-model pruning} transformation \cite{sen2009b} to prune  a large input meta-model such as the  {\UML} to a subset  called the effective input meta-model. \red{The application of metamodel pruning in the context of  model transformation testing leverages its use in a new perspective. The perspective is that of using an effective metamodel to consequently  generate  a tractable constraint satisfaction problem in {\Alloy}.} The effective input meta-model contains only classes, properties, their dependencies relevant the transformation under test. The often smaller effective input meta-model is transformed to a small formal representation in {\Alloy}. In contrast, transforming a large input meta-model such as the whole of {\UML} to {\Alloy} results in a formal model that renders SAT solving infeasible due to the large number of signatures or facts.
%	\item \textbf{Contribution 2:} We show how automatically generated test models can help  us improve a model transformation's pre-condition. For instance, the  test models we generate  for the case study transformation {\transfo} helps us discover new pre-condition constraints. These pre-conditions were not initially envisaged by the implementation of {\transfo}. The transformation {\transfo} was written in {\Kermeta} using the specification proposed at the  MTIP workshop \cite{bezivin2005}. We show that automatic generation can help us rapidly discover structures that human or even experts cannot preview in advance or require several years of transformation usage experience.
%	\item \textbf{Contribution 3:} We show that {\Cartier} consistently generates effective test models for a given strategy. We illustrate consistency by demonstrating that generating  multiple test models for the same test strategy does not  significantly change mutation scores. These test models correspond to multiple non-isomorphic solutions obtained using {\Alloy}'s symmetry breaking scheme \cite{ilya2001}. 
%\end{itemize}	
	
	 

%The paper is organized as follows. In Section \ref{sec:ProblemDescription} we present the transformation testing problem and the running case study. In Section \ref{sec:Foundation}, we present foundational ideas used in {\Cartier}. In  Section \ref{sec:Methodology}, we describe the {\Cartier} methodology for automatic test model generation. In Section \ref{sec:Experiments}, we present the experimental setup for test model generation using different strategies and discuss the results of mutation analysis. In Section \ref{sec:RelatedWork} we present related work. We conclude in Section \ref{sec:Conclusion}.
